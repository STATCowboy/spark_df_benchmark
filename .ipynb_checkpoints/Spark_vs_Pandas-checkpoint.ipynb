{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M1_9fdXdH5DU",
    "outputId": "55b0b24f-ed49-40b2-f0f9-a6b6fc8f4059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
      "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
      "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,392 kB]\n",
      "Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,163 kB]\n",
      "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,738 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,964 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,394 kB]\n",
      "Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [889 kB]\n",
      "Fetched 10.8 MB in 7s (1,643 kB/s)\n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
    "spark_version = 'spark-3.0.2'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ej7PkbGSH5iv"
   },
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demographicsFilter\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A-YIPc_-EWA3",
    "outputId": "40c3a23a-0a02-4dbf-f19d-de29f1d13658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 by 1\n",
      "3 loops, best of 5: 19.8 µs per loop\n",
      "3 loops, best of 5: 52.7 µs per loop\n",
      "3 loops, best of 5: 3.64 ms per loop\n",
      "3 loops, best of 5: 64 ms per loop\n",
      "The slowest run took 5.56 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3 loops, best of 5: 252 ms per loop\n",
      "1000 by 10\n",
      "3 loops, best of 5: 32 µs per loop\n",
      "The slowest run took 12.58 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3 loops, best of 5: 89.7 µs per loop\n",
      "3 loops, best of 5: 19.5 ms per loop\n",
      "3 loops, best of 5: 41.6 ms per loop\n",
      "3 loops, best of 5: 370 ms per loop\n",
      "1000 by 100\n",
      "The slowest run took 10.14 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3 loops, best of 5: 31.8 µs per loop\n",
      "3 loops, best of 5: 97.5 µs per loop\n",
      "3 loops, best of 5: 164 ms per loop\n",
      "3 loops, best of 5: 63.1 ms per loop\n",
      "3 loops, best of 5: 2.63 s per loop\n",
      "1000 by 1000\n",
      "3 loops, best of 5: 31.4 µs per loop\n",
      "3 loops, best of 5: 94.7 µs per loop\n",
      "3 loops, best of 5: 1.63 s per loop\n",
      "3 loops, best of 5: 164 ms per loop\n",
      "3 loops, best of 5: 26.6 s per loop\n",
      "10000 by 1\n",
      "3 loops, best of 5: 151 µs per loop\n",
      "3 loops, best of 5: 81.4 µs per loop\n",
      "3 loops, best of 5: 5.55 ms per loop\n",
      "3 loops, best of 5: 170 ms per loop\n",
      "3 loops, best of 5: 163 ms per loop\n",
      "10000 by 10\n",
      "3 loops, best of 5: 158 µs per loop\n",
      "3 loops, best of 5: 83.3 µs per loop\n",
      "3 loops, best of 5: 19.1 ms per loop\n",
      "3 loops, best of 5: 77.3 ms per loop\n",
      "3 loops, best of 5: 299 ms per loop\n",
      "10000 by 100\n",
      "3 loops, best of 5: 158 µs per loop\n",
      "3 loops, best of 5: 78.5 µs per loop\n",
      "3 loops, best of 5: 165 ms per loop\n",
      "3 loops, best of 5: 77.2 ms per loop\n",
      "3 loops, best of 5: 2.22 s per loop\n",
      "10000 by 1000\n",
      "3 loops, best of 5: 106 µs per loop\n",
      "3 loops, best of 5: 53 µs per loop\n",
      "3 loops, best of 5: 1.65 s per loop\n",
      "3 loops, best of 5: 198 ms per loop\n",
      "3 loops, best of 5: 26.2 s per loop\n",
      "100000 by 1\n",
      "3 loops, best of 5: 1.56 ms per loop\n",
      "3 loops, best of 5: 86.1 µs per loop\n",
      "3 loops, best of 5: 10.5 ms per loop\n",
      "3 loops, best of 5: 1.55 s per loop\n",
      "3 loops, best of 5: 194 ms per loop\n",
      "100000 by 10\n",
      "3 loops, best of 5: 1.57 ms per loop\n",
      "3 loops, best of 5: 87.5 µs per loop\n",
      "3 loops, best of 5: 23.3 ms per loop\n",
      "3 loops, best of 5: 617 ms per loop\n",
      "3 loops, best of 5: 258 ms per loop\n",
      "100000 by 100\n",
      "The slowest run took 4.03 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "3 loops, best of 5: 1.49 ms per loop\n",
      "3 loops, best of 5: 82.5 µs per loop\n",
      "3 loops, best of 5: 173 ms per loop\n",
      "3 loops, best of 5: 538 ms per loop\n",
      "3 loops, best of 5: 2.63 s per loop\n",
      "100000 by 1000\n",
      "3 loops, best of 5: 1.36 ms per loop\n",
      "3 loops, best of 5: 76.5 µs per loop\n",
      "3 loops, best of 5: 1.64 s per loop\n",
      "3 loops, best of 5: 686 ms per loop\n",
      "3 loops, best of 5: 27 s per loop\n",
      "1000000 by 1\n",
      "3 loops, best of 5: 15.3 ms per loop\n",
      "3 loops, best of 5: 86.4 µs per loop\n",
      "3 loops, best of 5: 37.2 ms per loop\n",
      "3 loops, best of 5: 15.5 s per loop\n",
      "3 loops, best of 5: 840 ms per loop\n",
      "1000000 by 10\n",
      "3 loops, best of 5: 10.5 ms per loop\n",
      "3 loops, best of 5: 52.8 µs per loop\n",
      "3 loops, best of 5: 72.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "column_counts=[1, 10, 100, 1000]\n",
    "element_counts=[1000, 10000, 100000, 1000000, 10000000]\n",
    "results=[]\n",
    "for each_element_count in element_counts: \n",
    "  for each_column_count in column_counts: \n",
    "    print(f'{each_element_count} by {each_column_count}')\n",
    "    result={}\n",
    "    result['element_count']=each_element_count\n",
    "    result['column_count']=each_column_count\n",
    "    ary=np.random.randint(100, size=each_element_count)\n",
    "    array_time=%timeit -o -n 3 np.random.randint(100, size=each_element_count)\n",
    "    result['array_time']=array_time.best\n",
    "    ary=ary.reshape(-1, each_column_count)\n",
    "    df=pd.DataFrame(ary)\n",
    "    df_time=%timeit -o -n 3 pd.DataFrame(ary)\n",
    "    result['pandas_df_time']=df_time.best\n",
    "    df_compute=%timeit -o -n 3 df.describe()\n",
    "    result['pandas_df_compute']=df_compute.best\n",
    "    spark_df=spark.createDataFrame(df)\n",
    "    spark_df_time=%timeit -o -n 3 spark.createDataFrame(df)\n",
    "    result['spark_df_time']=spark_df_time.best\n",
    "    # spark_df_compute=%timeit -o -n 3 spark_df.select(*[stddev(x) for x in spark_df.columns])\n",
    "    spark_df_compute=%timeit -o -n 3 spark_df.describe()\n",
    "    result['spark_df_compute']=spark_df_compute.best\n",
    "    results.append(result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tt1qjkOLiKIQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results_df=pd.DataFrame(results)\n",
    "results_df.to_csv('results_describe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "f2LGpGfn5GPH"
   },
   "outputs": [],
   "source": [
    "# # from pyspark.ml.linalg import Vectors\n",
    "# import pandas as pd\n",
    "# ary=np.random.randint(100, size=10000000)\n",
    "# ary=ary.reshape(-1, 2)\n",
    "# # vc=map(lambda x: (int(x[0]), Vectors.dense(x[1:])), ary)\n",
    "# # # vc=map(lambda x: int(x), ary)\n",
    "# # spark.createDataFrame(vc, schema=['number']).show()\n",
    "# df=pd.DataFrame(ary)#, columns=['number_1', 'number_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "mBPvaADh6a1M"
   },
   "outputs": [],
   "source": [
    "# spark_df=spark.createDataFrame(df)\n",
    "# # spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6oK239usGHXd",
    "outputId": "12ab32e1-da16-4928-8119-aa92b0c135cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4393859119991248"
      ]
     },
     "execution_count": 147,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_results.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3U9O9khZ7oxP",
    "outputId": "f6061e09-d071-495f-8828-da47987f54c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|avg(0)|\n",
      "+------+\n",
      "|49.269|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # from pyspark.sql.functions import mean\n",
    "# columns=spark_df.columns\n",
    "# means=map(lambda x: mean(x), columns)\n",
    "# # # print(*means)\n",
    "# # [mean(x) for x in spark_df.columns]\n",
    "# spark_df.select(*means).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLRzW2EP6xS-",
    "outputId": "62f08d29-687f-492e-88cc-c4b7fd588236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 14.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# # import org.apache.spark.sql.functions._\n",
    "\n",
    "# %%timeit -n 1\n",
    "# from pyspark.sql.functions import mean\n",
    "# # from pyspark.sql.functions import max\n",
    "# # spark_df.describe().show()\n",
    "# # spark_df.select([mean('0'), mean('0')]).show()\n",
    "# spark_df.select(*[mean(x) for x in spark_df.columns])#.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Spark_vs_Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
